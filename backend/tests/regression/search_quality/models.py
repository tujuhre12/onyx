from pydantic import BaseModel

from onyx.configs.constants import DocumentSource
from onyx.context.search.models import SavedSearchDoc


class GroundTruth(BaseModel):
    doc_source: DocumentSource
    doc_link: str


class TestQuery(BaseModel):
    question: str
    ground_truth: list[GroundTruth] = []
    categories: list[str] = []

    # autogenerated
    ground_truth_docids: list[str] = []


class EvalConfig(BaseModel):
    max_search_results: int
    max_answer_context: int
    num_workers: int
    request_timeout: int
    api_url: str
    search_only: bool
    rerank_all: bool


class OneshotQAResult(BaseModel):
    time_taken: float
    top_documents: list[SavedSearchDoc]
    answer: str | None


class RetrievedDocument(BaseModel):
    document_id: str
    content: str


class AnalysisSummary(BaseModel):
    question: str
    categories: list[str]
    found: bool
    rank: int | None
    total_results: int
    ground_truth_count: int
    answer: str | None = None
    response_relevancy: float | None = None
    response_groundedness: float | None = None
    faithfulness: float | None = None
    retrieved: list[RetrievedDocument] = []
    time_taken: float | None = None


class SearchMetrics(BaseModel):
    total_queries: int
    found_count: int

    # for found results
    best_rank: int
    worst_rank: int
    average_rank: float
    top_k_accuracy: dict[int, float]


class AnswerMetrics(BaseModel):
    average_response_relevancy: float
    average_response_groundedness: float
    average_faithfulness: float


class CombinedMetrics(SearchMetrics, AnswerMetrics):
    average_time_taken: float
