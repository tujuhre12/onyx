from datetime import datetime
from typing import cast

from langchain_core.runnables import RunnableConfig
from langgraph.types import StreamWriter

from onyx.agents.agent_search.dr.constants import MAX_CHAT_HISTORY_MESSAGES
from onyx.agents.agent_search.dr.constants import MAX_NUM_CLOSER_SUGGESTIONS
from onyx.agents.agent_search.dr.enums import DRPath
from onyx.agents.agent_search.dr.enums import ResearchType
from onyx.agents.agent_search.dr.models import AggregatedDRContext
from onyx.agents.agent_search.dr.models import TestInfoCompleteResponse
from onyx.agents.agent_search.dr.states import FinalUpdate
from onyx.agents.agent_search.dr.states import MainState
from onyx.agents.agent_search.dr.states import OrchestrationUpdate
from onyx.agents.agent_search.dr.utils import aggregate_context
from onyx.agents.agent_search.dr.utils import get_chat_history_string
from onyx.agents.agent_search.dr.utils import get_prompt_question
from onyx.agents.agent_search.dr.utils import parse_plan_to_dict
from onyx.agents.agent_search.models import GraphConfig
from onyx.agents.agent_search.shared_graph_utils.llm import invoke_llm_json
from onyx.agents.agent_search.shared_graph_utils.llm import stream_llm_answer
from onyx.agents.agent_search.shared_graph_utils.utils import (
    get_langgraph_node_log_string,
)
from onyx.agents.agent_search.shared_graph_utils.utils import write_custom_event
from onyx.agents.agent_search.utils import create_citation_format_list
from onyx.db.models import ChatMessage
from onyx.db.models import ResearchAgentIteration
from onyx.db.models import ResearchAgentIterationSubStep
from onyx.prompts.dr_prompts import FINAL_ANSWER_PROMPT_W_SUB_ANSWERS
from onyx.prompts.dr_prompts import FINAL_ANSWER_PROMPT_WITHOUT_SUB_ANSWERS
from onyx.prompts.dr_prompts import TEST_INFO_COMPLETE_PROMPT
from onyx.server.query_and_chat.streaming_models import CitationStart
from onyx.server.query_and_chat.streaming_models import MessageStart
from onyx.server.query_and_chat.streaming_models import OverallStop
from onyx.server.query_and_chat.streaming_models import SectionEnd
from onyx.utils.logger import setup_logger
from onyx.utils.threadpool_concurrency import run_with_timeout

logger = setup_logger()


def save_iteration(
    state: MainState, graph_config: GraphConfig, aggregated_context: AggregatedDRContext
) -> None:
    db_session = graph_config.persistence.db_session
    message_id = graph_config.persistence.message_id
    graph_config.persistence.chat_session_id
    research_type = graph_config.behavior.research_type
    db_session = graph_config.persistence.db_session

    # TODO: generate plan as dict in the first place
    plan_of_record = state.plan_of_record.plan if state.plan_of_record else ""
    plan_of_record_dict = parse_plan_to_dict(plan_of_record)

    chat_message = (
        db_session.query(ChatMessage).filter(ChatMessage.id == message_id).first()
    )
    if not chat_message:
        raise ValueError("Chat message with id not found")  # should never happen

    chat_message.research_type = research_type
    chat_message.research_plan = plan_of_record_dict

    for iteration_preparation in state.iteration_instructions:
        research_agent_iteration_step = ResearchAgentIteration(
            primary_question_id=message_id,
            reasoning=iteration_preparation.reasoning,
            purpose=iteration_preparation.purpose,
            iteration_nr=iteration_preparation.iteration_nr,
            created_at=datetime.now(),
        )
        db_session.add(research_agent_iteration_step)

    for iteration_answer in aggregated_context.global_iteration_responses:
        research_agent_iteration_sub_step = ResearchAgentIterationSubStep(
            primary_question_id=message_id,
            parent_question_id=None,
            iteration_nr=iteration_answer.iteration_nr,
            iteration_sub_step_nr=iteration_answer.parallelization_nr,
            sub_step_instructions=iteration_answer.question,
            sub_step_tool_id=iteration_answer.tool_id,
            sub_answer=iteration_answer.answer,
            reasoning=iteration_answer.reasoning,
            claims=iteration_answer.claims,
            cited_doc_results=create_citation_format_list(
                [doc for doc in iteration_answer.cited_documents.values()]
            ),
            additional_data=iteration_answer.additional_data,
            created_at=datetime.now(),
        )
        db_session.add(research_agent_iteration_sub_step)

    db_session.commit()


def closer(
    state: MainState, config: RunnableConfig, writer: StreamWriter = lambda _: None
) -> FinalUpdate | OrchestrationUpdate:
    """
    LangGraph node to close the DR process and finalize the answer.
    """

    node_start_time = datetime.now()
    # TODO: generate final answer using all the previous steps
    # (right now, answers from each step are concatenated onto each other)
    # Also, add missing fields once usage in UI is clear.

    current_step_nr = state.current_step_nr

    graph_config = cast(GraphConfig, config["metadata"]["config"])
    base_question = state.original_question
    if not base_question:
        raise ValueError("Question is required for closer")

    research_type = graph_config.behavior.research_type

    clarification = state.clarification
    prompt_question = get_prompt_question(base_question, clarification)

    chat_history_string = (
        get_chat_history_string(
            graph_config.inputs.prompt_builder.message_history,
            MAX_CHAT_HISTORY_MESSAGES,
        )
        or "(No chat history yet available)"
    )

    aggregated_context = aggregate_context(
        state.iteration_responses, include_documents=True
    )

    iteration_responses_string = aggregated_context.context
    all_cited_documents = aggregated_context.cited_documents

    num_closer_suggestions = state.num_closer_suggestions

    if (
        num_closer_suggestions < MAX_NUM_CLOSER_SUGGESTIONS
        and research_type == ResearchType.DEEP
    ):
        test_info_complete_prompt = TEST_INFO_COMPLETE_PROMPT.build(
            base_question=prompt_question,
            questions_answers_claims=iteration_responses_string,
            chat_history_string=chat_history_string,
            high_level_plan=(
                state.plan_of_record.plan
                if state.plan_of_record
                else "No plan available"
            ),
        )

        test_info_complete_json = invoke_llm_json(
            llm=graph_config.tooling.primary_llm,
            prompt=test_info_complete_prompt,
            schema=TestInfoCompleteResponse,
            timeout_override=40,
            # max_tokens=1000,
        )

        if test_info_complete_json.complete:
            pass

        else:
            return OrchestrationUpdate(
                tools_used=[DRPath.ORCHESTRATOR.value],
                query_list=[],
                log_messages=[
                    get_langgraph_node_log_string(
                        graph_component="main",
                        node_name="closer",
                        node_start_time=node_start_time,
                    )
                ],
                gaps=test_info_complete_json.gaps,
                num_closer_suggestions=num_closer_suggestions + 1,
            )

    # retrieved_search_docs = convert_inference_sections_to_search_docs(
    #     all_cited_documents
    # )

    write_custom_event(
        current_step_nr,
        MessageStart(
            id=str(graph_config.persistence.chat_session_id),
            content="",
            type="message_start",
        ),
        writer,
    )

    if research_type == ResearchType.THOUGHTFUL:
        final_answer_base_prompt = FINAL_ANSWER_PROMPT_WITHOUT_SUB_ANSWERS
    else:
        final_answer_base_prompt = FINAL_ANSWER_PROMPT_W_SUB_ANSWERS

    final_answer_prompt = final_answer_base_prompt.build(
        base_question=prompt_question,
        iteration_responses_string=iteration_responses_string,
        chat_history_string=chat_history_string,
    )

    try:
        # TODO: fix citations for non-document returning tools (right now, it cites iteration nr)
        streamed_output = run_with_timeout(
            240,
            lambda: stream_llm_answer(
                llm=graph_config.tooling.primary_llm,
                prompt=final_answer_prompt,
                event_name="basic_response",
                writer=writer,
                agent_answer_level=0,
                agent_answer_question_num=0,
                agent_answer_type="agent_level_answer",
                timeout_override=60,
                answer_piece="message_delta",
                ind=current_step_nr,
                # max_tokens=None,
            ),
        )

        final_answer = "".join(streamed_output[0])
    except Exception as e:
        raise ValueError(f"Error in consolidate_research: {e}")

    write_custom_event(current_step_nr, SectionEnd(), writer)

    current_step_nr += 1

    write_custom_event(current_step_nr, CitationStart(), writer)

    # write_custom_event(current_step_nr, CitationDelta(citations=retrieved_search_docs), writer)

    write_custom_event(current_step_nr, SectionEnd(), writer)

    current_step_nr += 1

    write_custom_event(current_step_nr, OverallStop(), writer)

    # Log the research agent steps
    save_iteration(state, graph_config, aggregated_context)

    return FinalUpdate(
        final_answer=final_answer,
        all_cited_documents=all_cited_documents,
        log_messages=[
            get_langgraph_node_log_string(
                graph_component="main",
                node_name="closer",
                node_start_time=node_start_time,
            )
        ],
    )
